---
layout: post
title: Symbol Balancing Markov Chains
canonical_url: https://medium.com/@glovguy/symbol-balancing-markov-chains-5cd095c2a1d5?source=rss-6b2723d73b48------2
tag:
- nlp
- machine-learning
- markov-chains
- python
---

<p>Machine learning is big these days, but before neural networks were all the rage, there were other statistical techniques being used to study language. One of these was the Markov chain.</p><p>Markov chains can be used to generate random text from trained text. At the basic level, it is a model of probabilistic state transitions. Given the system is in one state, how likely is it that it transitions into the next?</p><p>Markov chains are notoriously poor at certain things, especially following parse structure and symbol balancing. You might have some text generated by a Markov chain that contains meaningful phrases, but the way they are connected together doesn’t make a meaningful sentence. Often it will have a stray open quote or open parentheses that it doesn’t know it needs to close.</p><p>I wanted to see if I could take these sorts of things into account while still using a Markov model. My idea was to model these missing properties as state transitions. This would be similar to the way <a href="https://spacy.io/api/annotation#biluo">Spacy detects named entities</a>.</p><p>This post is going to walk through how to modify a Markov chain in python to take into account quotation mark balancing. The code for all of this can be found <a href="https://github.com/glovguy/markov">on my Github</a>.</p><h3>A Review: Markov Chains</h3><p>A Markov chain is just the description of any number of states and the probabilistic transitions that can occur from them. Given some series of words, it predicts the likely next word. It is typically made more accurate by making the state include more words.</p><p><a href="https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/markov_chains">Kahn Academy has a good explanation</a> of the math behind Markov chains. I also found <a href="http://www.cyber-omelette.com/2017/01/markov.html">this tutorial</a> on writing a Markov chain in python very informative. My implementation is based off of the one described there. I use Spacy to parse the input text, and use 2-grams rather than 1-grams.</p><p>First, we build a dictionary. The keys of this dictionary are states that the system can be in, and the values of those keys is an array of all the states that the system could transition into. This Markov chain uses 2-grams, so the dictionary keys are two words in a tuple.</p><pre>def build_chain(<strong>doc</strong>, <strong>chain</strong> <strong>=</strong> {}):<br>    index <strong>=</strong> 2<br>    doc_wo_spaces <strong>=</strong> [w <strong>for</strong> w <strong>in</strong> doc <strong>if</strong> <strong>not</strong> w.is_space]<br>    <strong>for</strong> word <strong>in</strong> doc_wo_spaces[index:]:<br>        key <strong>=</strong> (doc_wo_spaces[index <strong>-</strong> 2], doc_wo_spaces[index <strong>-</strong> 1])<br>        value <strong>=</strong> stringify(word)<br>        <strong>if</strong> key <strong>in</strong> chain:<br>            chain[key].append(value)<br>        <strong>else</strong>:<br>            chain[key] <strong>=</strong> [value]<br>        index <strong>+=</strong> 1<br>    <strong>return</strong> chain</pre><p>The input that this function takes is a Spacy doc, and an optional dictionary to build off of.</p><p>To generate text, we pick a random starting place. Then generating text is just a matter of finding the key that corresponds to the current state and selecting a random state to transition into.</p><h3>The Approach</h3><p>So here’s the tactic: encode quotes as a state, including a step-into and step-out-of state. The state for stepping into a quote is the quotation mark symbol. Consider the following sentence: <em>He said, “tell me more.” </em>The states for each token in that sentence are:</p><pre>He   said   ,    “      tell   me   more   .   ”<br>*     *     *   (in_”)  (“)    (“)  (“)   (“) (out_“)</pre><p>As you can see, the Markov chain “remembers” that it needs to close a quotation mark because it is in the quoted state after until it prints a close quote symbol.</p><p>One reasonable question to ask at this point is why have the state transitions be different from the state of being inside a quote? Why would we need to encode any additional information other than fact that a quote needs to be closed?</p><p>The reason we make a distinction of step-in and step-out states is to avoid an empty quotation. For example, imagine our Markov chain is generating text from a source text that has a lot of quotations. I was messing around using Plato’s Republic as a source text, and that book has many passages of dialogue between two characters. Consider:</p><blockquote>“So, If we look at a community coming into existence, we might see how justice originates.”<br>“I dare say.”</blockquote><p>If we are removing spaces, including line breaks, the quotation mark that ends the first quotation is followed immediately by the opening quotation mark to the second line of dialogue. The Markov chain would encode these two tokens as being adjacent.</p><p>Now imagine that our Markov chain has just generated some text and that the token it just generated is a quotation mark. It recognizes that it’s now in a quote state and should transition into another quote state. If you’re in a quote state, a symbol that happens to be extremely likely to be printed after that is another quotation mark! After all, many of the quotation marks at the end of quotes in our source text were followed by another quotation mark.</p><p>This means our text generator just printed: <strong>“”</strong>, which is obviously not meaningful. To avoid this, we encode the state transitions.</p><h3>Encoding the State</h3><p>This state transition will be encoded as a string included inside the state. In order to label the state, we will need:</p><ul><li>the locations of all the quote pairs that delimit a quotation, and</li><li>the ability to know whether a particular token is in between any of these quote pairs, or if it is one of the quote mark pairs.</li></ul><p>I’ll skim over the details of this, but roughly speaking I iterated over the whole document, made a list of all the token indices where there was an open and close quote pair. I stored an array of this data in the <strong>doc.user_data</strong> under the name <strong>‘balance_points’</strong>.</p><pre>( symbol,   index of open,   index of close )<br>( “,        7,               15             )</pre><p>Figuring out if a particular token is inside a quote is just a matter of checking if its index is in between any of these stored indices:</p><pre>def symbol_stack(<strong>token</strong>):<br>    bp <strong>=</strong> token.doc.user_data[&#39;balance_points&#39;]<br>    ti <strong>=</strong> token.i<br>    <strong>return</strong> <strong>sorted</strong>(<br>        [bal[0] <strong>for</strong> bal <strong>in</strong> bp <strong>if</strong> bal[1] <strong>&lt;=</strong> ti <strong>and</strong> bal[2] <strong>&gt;=</strong> ti],<br>        <strong>key=</strong>lambda <strong>bal</strong>: bal[1], <strong>reverse=</strong>True)</pre><p>You’ll notice that this function is returning a stack of these symbols, since it’s possible there can be nested quotes (e.g. someone quoting another person, or using quote marks to indicate a turn of phrase).</p><p>The label will just be the stack chain turned into a string, plus a prefix to indicate whether it is a transition in or out:</p><pre>def symbol_label(<strong>token</strong>):<br>    prefix <strong>=</strong> symbol_label_prefix(token)<br>    suffix <strong>=</strong> &#39;&#39;.join(symbol_stack(token))<br>    <strong>if</strong> prefix <strong>is</strong> <strong>not</strong> &#39;&#39;:<br>        <strong>return</strong> prefix <strong>+</strong> &#39;__&#39; <strong>+</strong> suffix<br>    <strong>else</strong>:<br>        <strong>return</strong> suffix</pre><pre>def symbol_label_prefix(<strong>token</strong>):<br>    balance_points <strong>=</strong> <strong>sorted</strong>(<br>        token.doc.user_data[&#39;balance_points&#39;], <br>        <strong>key=</strong>lambda <strong>bal</strong>: bal[1], <br>        <strong>reverse=</strong>True<br>    )<br>    ti <strong>=</strong> token.i<br>    <strong>for</strong> bal <strong>in</strong> balance_points:<br>        <strong>if</strong> bal[1] <strong>==</strong> ti:<br>            <strong>return</strong> &#39;in&#39;<br>        <strong>elif</strong> bal[2] <strong>==</strong> ti:<br>            <strong>return</strong> &#39;out&#39;<br>    <strong>return</strong> &#39;&#39;</pre><p>Here the prefix is either <strong>in__</strong> or <strong>out__</strong> for states that are transitions, or an empty string if there is no transition.</p><p>From this we can label any particular token as to whether it is inside a quote or not. We are ready to build the Markov chain!</p><h3>Building the Chain</h3><p>Here is the code for generating the dictionary to include this:</p><pre>def build_chain(<strong>doc</strong>, <strong>chain</strong> <strong>=</strong> {}):<br>    add_symbol_balance(doc)<br>    index <strong>=</strong> 2<br>    doc_wo_spaces <strong>=</strong> [w <strong>for</strong> w <strong>in</strong> doc <strong>if</strong> <strong>not</strong> w.is_space]<br>    <strong>for</strong> word <strong>in</strong> doc_wo_spaces[index:]:<br>        key <strong>=</strong> (<br>            doc_wo_spaces[index<strong>-</strong>2], <br>            doc_wo_spaces[index<strong>-</strong>1], <br>            symbol_label(doc_wo_spaces[index<strong>-</strong>1])<br>        )<br>        value <strong>=</strong> (stringify(word), symbol_label(word))<br>        <strong>if</strong> key <strong>in</strong> chain:<br>            chain[key].append(value)<br>        <strong>else</strong>:<br>            chain[key] <strong>=</strong> [value]<br>        index <strong>+=</strong> 1<br>    <strong>return</strong> chain</pre><p>This function works the same as the previous one; it takes a Spacy doc as an input and an optional dictionary. The function is <strong>add_symbol_balance(doc)</strong> is used to add the <strong>balance_points</strong> data to <strong>doc.user_data</strong>.</p><p>The important difference now is that the dictionary key is a tuple of two words (2-gram) as well as a <strong>symbol_label</strong>. The values of the dictionary also include a <strong>symbol_label</strong>.</p><h3>Conclusion</h3><p>For me this was primarily a proof of concept exercise. I wanted to try this idea out, and I think the result is pretty cool. It achieves symbol balancing in Markov chains in a way that isn’t hacky. This technique can be used to make more realistic chat bots, and could possibly be extended to more complicated problems than simple symbol balancing.</p><p>I will leave you with some text generated from my symbol-balancing Markov chain. It was trained on Plato’s <em>Apology</em>, <em>Symposium</em>, and <em>The Republic</em>.</p><blockquote>“I suppose not, no man is just of his own trade.” he said,”shall we ask him, only that he can tame the beasts.” “naturally.” “a good person to recite the words of a bed, but I fear it is beyond it and superior to it.” “apparently not.” “how?” “society originates because individuals can’t supply all their own proper functions which they excel at cooking and weaving too?” “it’s not something we can observe, the others mere sham.” “so that means ships and sailors and merchants.”</blockquote><p><a href="https://github.com/glovguy/markov">Github</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5cd095c2a1d5" width="1" height="1" alt="">
